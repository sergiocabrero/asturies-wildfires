import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from report import Report
import argparse

argparser = argparse.ArgumentParser(description='This scripts plots Social Network Analysis metrics')
argparser.add_argument('-i', help='Path of analysis data', dest='input', required=True)
argparser.add_argument('-o', help='Path to reports', dest='reports', required=True)

args = argparser.parse_args()

""" Set styles """
sns.set_style('white')

""" Report """
reportfile = args.reports+'/network_analysis.md'
report = Report('Network Analysis', reportfile)


""" Communication Range"""
report.print_line("## Communication Range")

fn_contacts = args.input+'/contacts_vs_range.csv'
fn_stats = args.input+'/wf_stats.csv'

df_contacts = pd.read_csv(fn_contacts, index_col=0)
df_stats = pd.read_csv(fn_stats, index_col=0)

df_contacts_norm = df_contacts.apply(lambda x: x/((df_stats.loc[df_stats.code==x.name,'sample_duration'].values[0]/30)*(df_stats.loc[df_stats.code==x.name, 'nnodes'].values[0]**2)))
mean = df_contacts_norm.mean(axis=1)
var = df_contacts_norm.var(axis=1)
maxi = df_contacts_norm.max(axis=1)
mini = df_contacts_norm.min(axis=1)

# plot_df = pd.DataFrame(zip(mean,var), columns=['mean', 'var'], index=mean.index)
fig = plt.figure(figsize=(3.5,2))
plt.plot(mean.index, mean.values, color='b')
# plt.fill_between(mean.index, mini, maxi, color='g', alpha=0.2)
plt.fill_between(mean.index, mean-var, mean+var, color='b', alpha=0.2)
plt.xlabel('Range (meters)')
plt.ylabel('Contact ratio')
report.print_figure(fig, 'contacts_vs_range', 'Contacts generated by increasing the communication range')
report.print_table_from_df(df_contacts_norm)
report.print_table_from_df(pd.DataFrame({'mean': mean, 'var': var, 'max': maxi, 'min': mini}))

""" coverage """
report.print_line("## Coverage")
fn_coverage = args.input+'/coverage_1km.csv'

df_coverage = pd.read_csv(fn_coverage)

mean_coverage = (df_coverage.groupby(['scenario', 'range']).area.mean()).reset_index()

fig = plt.figure(figsize=(3.5,3.5))
sns.swarmplot(data=mean_coverage, y='area', x='range')
plt.xlabel('Range (meters)')
plt.ylabel('Ratio of area covered')
plt.ylim(0,1)

report.print_figure(fig, 'coverage', 'Area covered by nodes')

""" Multihop routes """
report.print_line("## Multihop Routes")
fn_routes = args.input+'/route_lengths.csv'
df_routes = pd.read_csv(fn_routes)
df_routes.route_length = df_routes.route_length.astype('int')

route_norm_counts = df_routes.groupby('range').route_length.value_counts(normalize=True).rename('count_ratio').reset_index()

fig = plt.figure(figsize=(3.5,2))
ax = sns.barplot(data=route_norm_counts, x='route_length', y='count_ratio', hue='range')
plt.yscale('log')
plt.ylabel('Relative Frequency (Log Scale)')
plt.xlabel('Route length')
plt.legend(title='Range (meters)', loc='lower left', bbox_to_anchor=(0, 1), ncol=3)

report.print_figure(ax.get_figure(), 'route_length_frequency', 'Frequency of ocurrences of a given route length')
report.print_table_from_df(route_norm_counts)

""" Network density """
report.print_line("## Density")

# Load
fn_degrees = args.input+'/nodedegree.csv'
df_degrees = pd.read_csv(fn_degrees, index_col=0)
df_degrees.degree = df_degrees.degree.astype('int')

# normalize
degree_norm_counts = df_degrees.groupby('range').degree.value_counts(normalize=True).rename('count_ratio').reset_index()

# Plot
fig = plt.figure(figsize=(3.5,2))
ax = sns.barplot(data=degree_norm_counts, x='degree', y='count_ratio', hue='range')
plt.yscale('log')
plt.ylabel('Relative Frequency (Log Scale)')
plt.xlabel('Number of Neighbours')
plt.legend(title='Range (meters)', loc='lower left', bbox_to_anchor=(0, 1), ncol=3)

report.print_figure(ax.get_figure(), 'neighbour_frequency', 'Frequency of ocurrences of a given node degree (i.e. number of neighbours)')
report.print_table_from_df(degree_norm_counts)

""" Hubs """
report.print_line("## Hubs")
sns.distplot(df_degrees.groupby(['node', 'range']).degree.sum(), kde=False)
plt.yscale('log')
# Node mean degree

popularity = df_degrees.groupby(['scenario', 'range', 'node']).mean().reset_index()
fig = plt.figure(figsize=(3.5,2))
sns.violinplot(data=popularity, x='range', y='degree')
plt.xlabel('Range (meters)')
plt.ylabel('Mean number of neighbours')
plt.ylim(0,1.1*max(popularity.degree))

report.print_figure(fig, 'node_mean_degree', 'Mean degree of each node in the wildfire')
report.print_table_from_df(popularity)


# Popularity in each scenarios. Are there hubs in wfs?
popularity = pd.concat([df_routes.loc[:, ['node_A', 'range', 'scenario']].rename(columns={'node_A': 'node'}), df_routes.loc[:, ['node_B', 'range', 'scenario']].rename(columns={'node_B': 'node'})])
popularity = popularity.groupby(['range', 'scenario']).node.value_counts(normalize=True).rename('freq').sort_values().reset_index()

fig = plt.figure(figsize=(3.5,2))
sns.violinplot(data=popularity, x='range', y='freq')
#sns.swarmplot(data=popularity, x='range', y='freq', color="black", alpha=.4)
plt.xlabel('Range (meters)')
plt.ylabel('Popularity during wildfire')
plt.ylim(0,1.1*max(popularity.freq))

report.print_figure(fig, 'node_popularity', 'Relative popularity of each node in the wildfire')
report.print_table_from_df(popularity)

# standard deviation of popularity of nodes in scenario
popularity = pd.concat([df_routes.loc[:, ['node_A', 'range', 'scenario']].rename(columns={'node_A': 'node'}), df_routes.loc[:, ['node_B', 'range', 'scenario']].rename(columns={'node_B': 'node'})])
popularity = popularity.groupby(['range', 'scenario']).node.value_counts(normalize=True).rename('freq').sort_values().reset_index()
popularity = popularity.groupby(['range', 'scenario']).freq.max() - popularity.groupby(['range', 'scenario']).freq.min()
popularity = popularity.rename('frange').reset_index()

fig = plt.figure(figsize=(3.5,2))
#sns.violinplot(data=popularity, x='range', y='freq', inner=None)
sns.swarmplot(data=popularity, x='range', y='frange')
plt.xlabel('Range (meters)')
plt.ylabel('Popularity range during wildfire')
plt.ylim(0,1.1*max(popularity.frange))

report.print_figure(fig, 'node_popularity_range', 'Popularity range during a wildfire (more popular node - node less popular node)')
report.print_table_from_df(popularity)


# Overall node popularity
popularity = pd.concat([df_routes.loc[:, ['node_A', 'range', 'scenario']].rename(columns={'node_A': 'node'}), df_routes.loc[:, ['node_B', 'range', 'scenario']].rename(columns={'node_B': 'node'})])
popularity = popularity.groupby(['range']).node.value_counts(normalize=True).rename('freq').sort_values().reset_index()

fig = plt.figure(figsize=(3.5,2))
sns.violinplot(data=popularity, x='range', y='freq')
#sns.swarmplot(data=popularity, x='range', y='freq', color="grey", alpha=.4)
plt.xlabel('Range (meters)')
plt.ylabel('Popularity')
plt.ylim(0,1.1*max(popularity.freq))

report.print_figure(fig, 'overall_node_popularity', 'Relative popularity of each node in all wildfires')
report.print_table_from_df(popularity)


""" Network Connectivity """
report.print_line("## Connectivity")
import networkx as nx

overall_partition = df_routes.groupby(['range', 'scenario', 'node_A', 'node_B']).sum().reset_index()

def calculate_partitions(df):
    # nodes missing
    nodes_missing = df_stats.loc[df_stats.code == df.scenario.iloc[0], 'nnodes'].iloc[0] - len(pd.concat([df.node_A, df.node_B]).unique())
    edges = zip(df.node_A, df.node_B)
    G = nx.Graph()
    G.add_edges_from(edges)

    return nx.number_connected_components(G) + nodes_missing

partitions = overall_partition.groupby(['range', 'scenario']).apply(calculate_partitions).rename('partition').astype('int').reset_index()
partitions.loc[:, 'nnodes'] = partitions.scenario.map(df_stats.set_index('code').nnodes)
# Plot
fig = plt.figure(figsize=(3.5,2))
sns.swarmplot(x="partition", y="nnodes", data=partitions, hue='range')
plt.xlabel('Network partitions')
plt.ylabel('Nodes in the wildfire')
plt.legend(title='Range (meters)', loc='lower left', bbox_to_anchor=(0, 1), ncol=3)
report.print_figure(fig, 'overall_network_partitions', 'Network parititons if we considered the aggregated network graph')
report.print_table_from_df(partitions)

## Partitions with time
from math import ceil
time_partitions = df_routes
def set_time_period(df):
    # The +1 are to create the right bins (otherwise min or max create one for them)
    duration = max(df.time) - min(df.time) + 1
    # seconds start at 1
    percentage_time = (df.time - min(df.time) + 1)/duration
    percentage = 0.25
    timeperiod = percentage_time.apply(lambda x: int(ceil(x/percentage)))
    return timeperiod

time_partitions['timeperiod'] = time_partitions.groupby(['range', 'scenario']).apply(set_time_period).rename('timeperiod').reset_index().timeperiod

partitions = time_partitions.groupby(['timeperiod','range', 'scenario']).apply(calculate_partitions).rename('partition').astype('int').reset_index()
partitions.groupby('range').mean()

# Plot
fig = plt.figure(figsize=(3.5,2))
sns.pointplot(x="timeperiod", y="partition", data=partitions, hue='range')
ax = sns.swarmplot(x="timeperiod", y="partition", data=partitions, hue='range', alpha=0.4)
# Fix duplicated legend
handles, labels = ax.get_legend_handles_labels()
l = plt.legend(handles[0:3], labels[0:3], title='Range (meters)',  loc='lower left', bbox_to_anchor=(0, 1), ncol=3)

plt.xlabel('Period')
plt.ylabel('Network partitions')
plt.ylim(0,max(partitions.partition)+1)
report.print_figure(fig, 'timequartil_network_partitions', 'Network parititons if we considered the aggregated network graph in 0.25 increments')
report.print_table_from_df(partitions)
